version: "3.3"
services:
  namenode:
    image: apache/hadoop:3
    container_name: namenode
    hostname: namenode
    ports:
      - 9870:9870 
    env_file:
      - ./config
    environment:
      ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
    command: ["hdfs", "namenode"]
    volumes:
      - ./hadoop-config:/opt/hadoop/etc/hadoop
      - ./hadoop-config/log4j.properties:/opt/hadoop/etc/hadoop/log4j.properties

  datanode-1:
    image: apache/hadoop:3
    container_name: datanode-1
    command: [ "hdfs", "datanode" ]
    env_file:
      - ./config
    volumes:
      - ./hadoop-config:/opt/hadoop/etc/hadoop

  datanode-2:
    image: apache/hadoop:3
    container_name: datanode-2
    command: [ "hdfs", "datanode" ]
    env_file:
      - ./config
    volumes:
      - ./hadoop-config:/opt/hadoop/etc/hadoop
      
  resourcemanager:
    image: apache/hadoop:3
    container_name: resourcemanager
    hostname: resourcemanager
    command: [ "yarn", "resourcemanager" ]
    ports:
      - 8088:8088
    env_file:
      - ./config
    volumes:
      - ./test.sh:/opt/test.sh
 
  nodemanager:
    image: apache/hadoop:3
    container_name: nodemanager
    command: [ "yarn", "nodemanager" ]
    env_file:
      - ./config

  spark-master:
    image: cluster-apache-spark:3.0.2
    container_name: spark-master
    user: root 
    hostname: spark
    ports:
      - "9090:8080"
      - "7077:7077"
    volumes:
        - ./hadoop-config:/opt/hadoop/etc/hadoop
        - ./src/spark/apps:/opt/spark-apps
        - ./src/spark/data:/opt/spark-data
        - ./src/spark/applications:/usr/local/spark/applications            
        - ./src/spark/assets:/usr/local/spark/assets 
    environment:
      - SPARK_LOCAL_IP=spark-master
      - SPARK_WORKLOAD=master
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no

  spark-worker-1:
    image: cluster-apache-spark:3.0.2
    container_name: spark-worker-1
    user: root
    hostname: spark-worker-1
    ports:
      - "9091:8080"
      - "7000:7000"
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker-1
    volumes:
        - ./hadoop-config:/opt/hadoop/etc/hadoop
        - ./src/spark/apps:/opt/spark-apps
        - ./src/spark/data:/opt/spark-data
        - ./src/spark/applications:/usr/local/spark/applications            
        - ./src/spark/assets:/usr/local/spark/assets 

  spark-worker-2:
    image: cluster-apache-spark:3.0.2
    container_name: spark-worker-2
    user: root
    hostname: spark-worker-2
    ports:
      - "9092:8080"
      - "7001:7000"
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker-2
    volumes:
        - ./hadoop-config:/opt/hadoop/etc/hadoop
        - ./src/spark/apps:/opt/spark-apps
        - ./src/spark/data:/opt/spark-data
        - ./src/spark/applications:/usr/local/spark/applications            
        - ./src/spark/assets:/usr/local/spark/assets 

  postgres:
    image: postgres:15
    container_name: postgresdb
    ports: 
      - "5432:5432"
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - ./postgres_data:/var/lib/postgresql/data

  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: pgadmin
    ports:
      - "5050:80"
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: admin
    depends_on:
      - postgres

  redis:
    image: redis:latest
    container_name: redis


  airflow-init:
    image: apache/airflow:2.7.1
    container_name: airflow-init
    depends_on:
      - postgres
      - redis
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: postgresql+psycopg2://airflow:airflow@postgres/airflow
    command: >
      bash -c "
      until airflow db check; do
        echo 'Waiting for database connection...';
        sleep 5;
      done;
      airflow db init &&
      airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com
      "
    restart: "no"

  airflow-webserver:
    hostname: airflow
    container_name: airflow-webserver
    image: apache/airflow:2.7.1
    restart: always
    depends_on:
        - postgres
        - spark-master
        - spark-worker-1
        - spark-worker-2
    environment:   
        AIRFLOW__CORE__EXECUTOR: CeleryExecutor
        AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
        AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
        AIRFLOW__CELERY__RESULT_BACKEND: postgresql+psycopg2://airflow:airflow@postgres/airflow
        AIRFLOW__WEBSERVER__PORT: 8080
        LOAD_EX: n
        EXECUTOR: Local
        AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
    volumes:
        - ./airflow-data:/opt/airflow/data
        - ./src/dags:/opt/airflow/dags
        - ./src/spark/applications:/usr/local/spark/applications            
        - ./src/spark/assets:/usr/local/spark/assets     
    ports:
        - "8085:8080"
    healthcheck:
        test: ["CMD-SHELL", "[ -f /opt/airflow/airflow-webserver.pid ]"]
        interval: 30s
        timeout: 30s
        retries: 3
    command: webserver
    

  airflow-scheduler:
    image: apache/airflow:2.7.1
    container_name: airflow-scheduler
    depends_on:
      - postgres
      - redis
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: postgresql+psycopg2://airflow:airflow@postgres/airflow
    volumes:
      - ./airflow-data:/opt/airflow/data
      - ./src/dags:/opt/airflow/dags
      - ./src/spark/applications:/usr/local/spark/applications            
      - ./src/spark/assets:/usr/local/spark/assets     
    command: scheduler

  # airflow-worker:
  #   image: apache/airflow:2.7.1
  #   container_name: airflow-worker
  #   depends_on:
  #     - postgres
  #     - redis
  #   environment:
  #     AIRFLOW__CORE__EXECUTOR: CeleryExecutor
  #     AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
  #     AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
  #     AIRFLOW__CELERY__RESULT_BACKEND: postgresql+psycopg2://airflow:airflow@postgres/airflow
  #   networks:
  #     - airflow_network
  #   command: celery worker

networks:
  default:
    driver: bridge

volumes:
    postgres:
    airflow-data: